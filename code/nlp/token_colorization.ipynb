{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df18ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d76fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# this program is a test of tiktoken, and its ability to separate paragraphs into colored tokens\n",
    "# based on a selected LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a620682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee920fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09595a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "394316b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6160c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2852f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2bb9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_print(filename):\n",
    "    with open(\"hamlet.txt\", \"r\") as file:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "    text_obj = Text()\n",
    "    colors = [\"cyan\", \"magenta\", \"yellow\", \"green\", \"red\" \"blue\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b72a35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c96a5184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_print(filename):\n",
    "    with open(\"hamlet.txt\", \"r\") as file:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "    text_obj = Text()\n",
    "    colors = [\"cyan\", \"magenta\", \"yellow\", \"green\", \"red\" \"blue\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c4a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_print(filename):\n",
    "    with open(\"hamlet.txt\", \"r\") as file:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "    text_obj = Text()\n",
    "    colors = [\"cyan\", \"magenta\", \"yellow\", \"green\", \"red\" \"blue\"]\n",
    "    for i, token in enumerate(token_strings):\n",
    "        # simply cylce through the colors\n",
    "        color = colors[i % len(colors)]\n",
    "        text_obj.append(token, syle=color)\n",
    "    # should print in color\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0e0de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_print(\"hamlet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6f8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_print(filename):\n",
    "    with open(\"hamlet.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "    text_obj = Text()\n",
    "    colors = [\"cyan\", \"magenta\", \"yellow\", \"green\", \"red\" \"blue\"]\n",
    "    for i, token in enumerate(token_strings):\n",
    "        # simply cylce through the colors\n",
    "        color = colors[i % len(colors)]\n",
    "        text_obj.append(token, syle=color)\n",
    "    # should print in color\n",
    "    console.print(text_obj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5ac9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_print(\"hamlet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8bc872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_print(filename):\n",
    "    with open(\"hamlet.txt\", \"r\") as file:\n",
    "        text = file.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "    text_obj = Text()\n",
    "    colors = [\"cyan\", \"magenta\", \"yellow\", \"green\", \"red\" \"blue\"]\n",
    "    for i, token in enumerate(token_strings):\n",
    "        # simply cylce through the colors\n",
    "        color = colors[i % len(colors)]\n",
    "        text_obj.append(token, style=color)\n",
    "    # should print in color\n",
    "    console.print(text_obj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6782dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_print(\"hamlet.txt\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
